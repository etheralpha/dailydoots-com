---
layout: markdown
date: 2025-05-30
episode: 113
guest: Ram Ahluwalia
guest_topic: "[Lumida Wealth](https://www.lumidawealth.com/), a digitally native, SEC registered investment advisor specializing in alternative investments and digital assets"
weekly_link: https://reddit.com/r/ethereum/s/iWQRJbbBtQ
podcast_link: https://www.youtube.com/watch?v=cJzbCiVAooE
poap_link: 
---


<details markdown=1>
<summary>The morning roundup</summary>
[View on Reddit →](https://reddit.com/r/ethereum/comments/1kywv4z/comment/mv0tm11/)

[u/FrenktheTank](https://reddit.com/u/FrenktheTank)

> Ethereum

[u/Weitarded](https://reddit.com/u/Weitarded)

> $2,640.28

[u/TimbukNine](https://reddit.com/u/TimbukNine)

> 0.02481

</details>
<details markdown=1>
<summary>Weekly Haiku: u/Jey_s_TeArS</summary>
[View on Reddit →](https://reddit.com/r/ethereum/comments/1kx9pdx/comment/mus1tpv/)

*Epoch committee,*

*The attestation duty,*

*Staking node witty.*

</details>
<details markdown=1>
<summary>Updates: u/ChomKy_W0mpii</summary>
[View on Reddit →](https://reddit.com/r/ethereum/comments/1ky3d0m/daily_general_discussion_may_29_2025/muxp4a8/)

**Day 93 of BTCS Inc. eth updates**

- The Ethereum Foundation announced staking 50,000 $RAIL tokens, allowing participation in RAILGUN governance and claiming protocol fee rewards. [source](https://x.com/RAILGUN_Project/status/1928070100084506938)
- At ETHPrague, Vitalik Buterin discussed AI's potential, suggesting it should complement human evolution rather than compete. He highlighted its use in governance and the need for balanced technological integration. 

**\[L1 Ethereum Transactions Per Day\]**

**1.387M** transactions/day for May 28 2025 **up** from 1.105M from one year ago

</details>
<details markdown=1>
<summary>Shitpost of the week: u/Twelvemeatballs</summary>
[View on Reddit →](https://reddit.com/r/ethereum/comments/1ku4n9g/daily_general_discussion_may_24_2025/mtzjx54/)

(This is fiction and not at all based on my personal experience with airdrops)

# Everyone Got Something But You

⚠ Travel Warning: Q3 Advisory

Do not attempt to cross the claim fields unprepared. The terrain appears welcoming: flat, accessible, even generous. This is a lie. The air is full of expectations and wallet dust. The locals are feral. The newcomers are worse.

If you go, bring sunscreen, plausible deniability, and three forms of contribution. Leave your dignity at the gate. You won't need it.

This is where the airdrops landed. Or rather, detonated. Tokens scattered across wallets like pollen, or shrapnel. Eligibility was whispered, not declared. Some said it favored contributors. Others said it punished activity that looked too much like affection. The algorithm tried to filter out sybils but in the process, it unpersoned half the chain.

Someone lived here, once. Briefly. Airdrops simply appeared, like wild garlic in the forest, free for the taking, if you were at the right place at the right time.

Then the pilgrims began arriving, clutching half-completed quests and spreadsheets of transaction hashes. They brought receipts. It didn't matter. The logic had already calcified. One man spun up 214 wallets in a single night and still qualified for nothing. Another posted a thank-you gif and received governance. No one knows why. If you were miscategorized, you stayed that way. Mostly what came back was silence and a handful of worthless tokens.

The ghosts wander still, clutching wallets that once held weight, insisting that they were early contributors. True believers, in it for the tech. Gamblers, who dreamed of lambos and received only sodium packets. The tokens unlocked too late and meant too little.

The gardens are long gone. Launch promises scatter across the dusty plains like tumbleweeds.

They say that if you idle too long, you become a watch-only address in your own story.

The claim fields know who you are. Caring was never part of the protocol.

</details>
<details markdown=1>
<summary>u/Fiberpunk2077 asks who will run provers and the arincentives and u/haurog answers</summary>
[View on Reddit →](https://reddit.com/r/ethereum/comments/1krqo5y/daily_general_discussion_may_21_2025/mth2cq5/)

[u/Fiberpunk2077](https://reddit.com/u/Fiberpunk2077):

Exciting about the huge step on real-time proving, but I have a couple questions for those more knowledgeable.

What is the (financial) incentive to run the prover cluster? I assume it is the same as current block builders (i.e., MEV)? Do we have a sense of how much infrastructure costs are now for block builders?

Is the assumption the current block builders would migrate to real-time proving?

How many provers do we need to ensure liveliness and censorship resistance? I'm assuming functionality will remain to build blocks locally and verify, but that would, in effect, basically be stopping the chain once we are used to 1000x throughput..

---

[View on Reddit →](https://reddit.com/r/ethereum/comments/1krqo5y/daily_general_discussion_may_21_2025/mtijqq4/)

[u/haurog](https://reddit.com/u/haurog):

Block building and block proofing are two different jobs. In my current understanding the plan is to publish a block on chain and then give the provers until the next slot to publish a proof. There might be some advantage to be a fully integrated builder and prover, but it probably is not a big advantage and therefore there might be enough room for an independent market to develop. I do not know about any discussion about remuneration and how it will work for prover nodes, but I could imagine they are getting paid when their proof is published on chain. As far as I remember the proving costs per block should be extremely low. I think on the order of cents.

At the moment there is already a zk prover market for all the different zk L1s and L2s. The market still is very immature, but it sounds like there is a lot of progress and different models are being tested. A recent zk podcast covers one of the approaches and the history a bit: <https://zeroknowledge.fm/podcast/355/>

As provers have almost no power over the network we will just need 1 of them to do their job for the chain to work normally. If they do not do their job and fail to publish proofs in time, your smartwatch is not able to verify it and trustlessly retrieve the chain state. Not good but also not chain breaking. Depending on how far things are pushed one might easily be able to rent a handful of old GPUs on one of the decentralized GPU market places and build proofs as well. Currently these markets are far far cheaper than anything a professional hyperscaler can provide. Sure you rent potatoes, but handful of the potatoes of the future might be enough to make proofs. If we push things to the absolute limit however, then only the most sophisticated entities can provide proofs which is a centralization risk.

For censorship resistance we need the block builders to follow certain rules. FOCIL is such a proposal which is being heavily discussed and will come in a year or two at the earliest.

There will be medium nodes in the network, which are similar to the current validators recomendations. My 2 year old NUC is able to validate 1k to 10k tps. Not too far away from still being able to validate a highly scaled and zk proven chain. In a few years with newer hardware and more optimizied clients I guess we could be able to validate the chain even without zk proofs. So, I am not too worried about this issue at the moment.

</details>
<details markdown=1>
<summary>u/pa7x1 notices an interesting change in block size after Pectra</summary>
[View on Reddit →](https://reddit.com/r/ethereum/comments/1ksjj96/daily_general_discussion_may_22_2025/mtmo1kn/)

<https://etherscan.io/chart/blocksize>

This is a very interesting chart. You can see that with blobs we reduced significantly the size of blocks. Of course this just pushed that bandwidth somewhere else but also prevented state growth.

With the latest upgrade in Pectra (post May 7th) you can see that some spikes in block size have been tamed. We went from having regular spikes of 120 Kbyte blocks to having very regular 90 Kbyte blocks. That gives us a 33% increase today simply by placing blocksizes closer to how they were pre-Pectra.

So even the most conservative of validators could quite confidently signal 50 MGas because they were already operating at those levels pre-Pectra.

</details>
<details markdown=1>
<summary>u/cobber1211 shares a unique protocol built on Ethereum</summary>
[View on Reddit →](https://reddit.com/r/ethereum/comments/1ktcbmc/daily_general_discussion_may_23_2025/mtskr3k/)

Reposting here because I don't want my reply to get lost under a post with 4 upvotes, also because I want to shill my favorite project built on Ethereum

I'm surprised Kleros doesn't get more attention on this sub. Maybe because they don't market much and it's pretty low Mcap (for now at least).

But for those who don't know, it's an arbitration protocol where jurors (token stakers) rule on disputes. Any time there's a dispute between multiple parties, they can raise it with Kleros, which creates a case in which the parties submit evidence to argue their side of things. The case is then sent out to a randomly selected pool of jurors to vote on, with the majority outcome winning. The protocol can be applied in any sort of situation that requires arbitration.

Obviously this model raises a lot of questions, for example around preventing juror corruption and collusion,   lazy voting, 51% attacks, etc. I won't go into all that here, but I will say that the protocol has a lot of cleverly designed mechanisms for mitigating these things - allowing for appeal rounds, built-in token forking, carefully tuned juror rewards/slashing, a hierarchical court system - that address all of the common concerns I see raised. The idea is credible enough for [Vitalik](https://x.com/Kleros_io/status/1473632145256165378) to have [shilled it](https://x.com/Kleros_io/status/1847804599203238242) multiple [times](https://x.com/Kleros_io/status/1847804599203238242); he's called it "a really valuable and important piece of infrastructure for the Ethereum ecosystem".

Ultimately, the aim is to provide a resolution process that is significantly faster and cheaper than traditional arbitration, without sacrificing any of the accuracy or fairness.

Unlike some protocols which depend on hype and speculation that they might some day be useful, Kleros is already demonstrating real-world value. It has ruled on over 1500 cases at this point. Historically, these cases have mostly involved purely on-chain applications: e.g. on-chain insurance, digital identity curation, prediction market oracles. But it is [increasingly dipping its toes](https://blog.kleros.io/kleros-enterprise/) into the trillion-dollar, real-world arbitration market: they have recently started a trial with MetLife, one of the largest global insurance providers, and [another trial](https://x.com/Kleros_io/status/1799158478537187415) with local government in Mendoza for resolving neighborhood and consumer disputes.

And for not-just-a-trial stuff that's actually being used in business: it's currently integrated with lemon.me, a big South American crypto exchange, and has been used to resolve over 100 consumer claims. Compared to Lemon's process for claims, Kleros has improved user retention and satisfaction, lowered resolution time, and lowered dispute costs.

Also some notes:

- The only real competitors in the arena of "subjective oracles" (e.g. UMA) are pretty unsophisticated: usually they just copy-and-paste the DAO model without any considerations made for the peculiarities of dispute resolution, which often leads to [disaster](https://x.com/Domahhhh/status/1905258165777596812).

- V2 of the protocol is launching very soon with a wealth of new features, esp. significantly lower gas fees due to migrating from Ethereum mainnet to Arbitrum. This will enable a massive number of low-value, high-volume use cases that were not previously possible.

tl;dr kleros good

</details>
<details markdown=1>
<summary>u/Flashy-Butterfly6310 discusses the value drivers behind ETH</summary>
[View on Reddit →](https://reddit.com/r/ethereum/comments/1kuvjof/daily_general_discussion_may_25_2025/mu5yvh8/)

I am a strong Ethereum believer and, by extension, the value of ETH itself.

One of the main criticisms I often hear is about ETH’s ability to actually capture the value created within this ecosystem (see [this example at 1:04:56](https://youtu.be/prgRUFtmaY4?si=pcozvw9uEo17q8Jk&t=1h4m56s0) in French ).

To sum up the criticism: it’s often argued that Ethereum, like the early open-source internet protocols (such as TCP/IP, HTTP, FTP, SMTP, SSL, etc.), will ultimately not capture much of the value generated on top of it. According to this view, most of the value will be captured by applications and companies built on top of the platform (e.g. websites, services like Amazon built on top of the Internet), rather than by ETH, the protocol’s native asset.

However, I believe Ethereum is fundamentally different in this regard. Ethereum has built-in value capture mechanisms that the original internet protocols never had. The most important of these is the utility of ETH itself: it is required to pay for transaction fees (gas), secure the network through staking, and it is increasingly used as a trustless form of money within the ecosystem. These mechanisms create direct and ongoing demand for ETH, allowing the protocol and its native asset to capture a meaningful share of the value generated by activity on the platform.

**How do you respond (or would you respond) to someone bringing up this kind of argument?**

**What are the specific features or mechanisms that allow Ethereum to capture value?**

**How is it different from other open-source protocols that serve as infrastructure for applications, when it comes to capturing value from the ecosystem?**

</details>
<details markdown=1>
<summary>u/physalisx explains why we're still waiting for no-approval swaps post Pectra</summary>
[View on Reddit →](https://reddit.com/r/ethereum/comments/1kuvjof/daily_general_discussion_may_25_2025/mu4z7cc/)

[Yeah, yeah.](https://reddit.com/r/ethereum/comments/1kleo0a/daily_general_discussion_may_13_2025/ms4srma/) We're going to keep seeing confused users about this. And there's many thousands who are confused but don't even come here...

With Pectra came just the backend side - it's supported by the protocol now to batch approve+swap into one tx. But utilizing this still needs to be implemented into wallets and dapps. 

Metamask should support it already, and Uniswap too. Who else? Who knows! Guess we have to follow the social media accounts for all wallets and dapps now to know when we can actually use this feature.

It's a pretty big coordination and marketing failure imo. Users don't care if "the protocol is ready". Ethereum isn't just the protocol. Claiming ["no more "approve + confirm" loops. no more double signing for every action."](https://x.com/ethereum/status/1919794621132968240) 1 day before Pectra was a setup for disappointment.

</details>
<details markdown=1>
<summary>u/eth2353 shares a report on the risks of pumping the gas and u/samcm, one of the authors, weighs in</summary>
[View on Reddit →](https://reddit.com/r/ethereum/comments/1kwfypq/daily_general_discussion_may_27_2025/muhuj1c/)

[u/eth2353](https://reddit.com/u/eth2353):

[A great blog post was published today](https://ethpandaops.io/posts/60m-gas-sepolia-hoodi/) by the EF DevOps team (ethPandaOps) that analyses the impact of recent gas limit increases (to 60M) on the Sepolia and Hoodi testnets.

It's well worth a read for anyone that runs validators or is interested in seeing the L1 scale through gas limit increases ( like u/Weitarded and about 150k validators on Ethereum currently signaling for a 60M gas limit ).

This is exactly the kind of analysis that imo should be included in each and every post calling for higher gas limits, instead of almost blindly calling for a higher number. We already have 15% of the network signaling for 60M on mainnet, and we don't even have a rough idea of how safe that is yet...

Notably:

> There is a large difference between the two networks on both metrics. This may indicate a sensitivity to large execution state size. This is a particularly interesting result as Mainnet has a much larger execution state size and we'll be monitoring this closely as we continue to scale.

Mainnet Ethereum has a large state, and with an increased gas limit it will grow even faster. This is important to keep in mind when considering higher gas limits before we have things like state expiry. We can't push the gas limit into the sky just because we can _execute_ large blocks quickly enough. Apart from the issues surrounding state growth, we also need to account for worst-case blocks that are specifically constructed by an attacker to take as long as possible to execute, a sort of DoS attack.

To wrap up, here's the conclusion from the team:

> Based on the data from Hoodi & Sepolia, 60M is safe as far as block/blob propagation is concerned. It's very important to note that these testnets are not representative of Mainnet. We'll be conducting additional analysis on Mainnet in the coming days, but for now we can say that 60M is possible on a fundamental level.

Sidenote: this may sound like I'm personally strongly against a gas limit increase, but that's not the case. I just want it to be done with great care.

---

[View on Reddit →](https://reddit.com/r/ethereum/comments/1kwfypq/daily_general_discussion_may_27_2025/mui49pa/)

[u/samcm](https://reddit.com/u/samcm):

Thanks for linking the post! It was a fun one to dive in to 😅

> ( like u/Weitarded and about 150k validators on Ethereum currently signaling for a 60M gas limit ).

Given the analysis, and combined with the fact that we sat at 30M for so long, I personally think 60M is safe. I'd much prefer that we went to 45M first, and then 60M, but coordinating these changes has a large overhead. 

In saying that, **60M should be the absolute max for Pectra**. There's a handful of scaling related EIPs scheduled for inclusion in Fusaka, and we must wait for them before pushing beyond 60M.

> This is exactly the kind of analysis that imo should be included in each and every post calling for higher gas limits

Completely agree. While we'll continue to do this style of post, I'd really like to encourage others to also dive in to the data if they're interested. [All of our data is published freely!](https://ethpandaops.io/data/xatu)

Shameless side note/shill: we're trying to really scale up our data-driven approach to making decisions and that means we're looking for more data contributors. None of these analysis posts are possible without those users who contribute their data, and the dataset is starting to become an invaluable resource for core devs and researchers. If you're running a node and are interested in contributing you can learn more [here](https://ethpandaops.io/contribute-data)

</details>
<details markdown=1>
<summary>u/haurog discusses the drawbacks and limitations in pumping the gas</summary>
[View on Reddit →](https://reddit.com/r/ethereum/comments/1kvn3n3/daily_general_discussion_may_26_2025/mub0ya9/)

Drawback are state growth, history growth, bandwidth limitations and, to a lesser extent, CPU usage. The biggest one is probably bandwidth, which might get addressed in the upgrade after Fusaka with ePBS. ePBS also helps with CPU usage, as it allows the full 12s slot time for running all the transactions. History growth is starting to get addressed in the coming weeks with history expiry. State growth is unfortunately a bit further out as the verkle tree upgrade have most probably been scrapped and is currently being redesigned. 

The difference between 60 million and a 100 million is not that huge, so, there is in my opinion not a simple argument to make for one or the other. 60 million is mostly what people talk about and is currently being tested on the testnets. If you go up to 150 million I am currently of the opinion that this should only be done once we have ePBS because of the bandwidth constraints some nodes have. As we will most probably also decrease slot times in the future, which in effect increases the throughput without increasing the gas limits, it is unwise to push the gas limit to their absolute limits now which would make it necessary to reduce them again later on. And yes I am signalling for 60 million since the Pectra upgrade. 

I wrote a much longer piece a few months ago, most should still be valid: <https://reddit.com/r/ethereum/comments/1ikhihv/daily_general_discussion_february_08_2025/mbn7z7b/>

</details>
<details markdown=1>
<summary>u/Twelvemeatballs got a lot of hope from talks at day 2 of ETHPrague</summary>
[View on Reddit →](https://reddit.com/r/ethereum/comments/1kx9pdx/daily_general_discussion_may_28_2025/muq26cn/)

Day two of ETHPrague and if possible, even better presentations than yesterday. The highlight for me was hands down Tim Berners-Lee being quizzed by Vitalik Buterin. TB-L talked about how it was all meant to be decentralised (anyone can run a webserver, anyone can have a website) but that's not how it turned out. Was great to see them bouncing ideas back and forth and talking about how things could look/should look for Web3. I also attended Aya Miyagotchi's fireside talk where she got EF issues out of the way at the start and then talked with Christopher Fabian of Giga about how they got Unicef on board. This connected nicely to Maurice Chiodo, a Cambridge maths professior,  talking about ethics of AI and blockchain and companies who arent quite sure what they are doing yet... followed by our own Paul Brody, who  injected the crowd  with a large dose of hopium for the business adoption of stablecoins and smart contracts. 

It's hard to be worried about the future of Ethereum after hearing all these great minds talking about what they've done and what they expect of the future.  I even loved Joachim Schwerin talking Web3 in the EU; he's a huge advocate of privacy and seems like exactly the kind of voice that Euro~~poors~~peans need to have representing the space (also quickly dropped a word in favour of TornadoCash!). 

I'm looking forward to writing more detail once I'm home and I'm really hoping that the talks will all be available on video (they don't seem to have confirmed that yet so I've taken tons of notes). 

Tomorrow is mostly  ETHGlobal Pragma though I'll sneak back in the afternoon for one or two more ETHPrague talks.  Then I am going to quietly collapse for a day.

</details>
<details markdown=1>
<summary>u/haurog just discovered how easy it is to permissionlessly force include a transaction on an L2</summary>
[View on Reddit →](https://reddit.com/r/ethereum/comments/1kvn3n3/daily_general_discussion_may_26_2025/muayy2f/)

Today I learned that force including transactions into rollups is super easy. I recently withdrew some funds from unichain and used one of the canonical bridge interfaces suggested by unichain (brid.gg). If you click on the config symbol on their interface you can enable expert mode which allows you to force transaction onto the rollups. They have 'force transfer', 'force withdrawal' and 'custom transaction'. This means you can make a transaction on Ethereum L1 which then forces the sequencer to include whatever you you said they should do. You can make a fully custom transaction like withdraw your funds from a pool or do a swap or ... . This is a very important piece to to prevent censorship on the rollup even though there is only one entity controlling the sequencer. 

I was not aware that there exists such a nice interface already. I tried it with Base and my forced transaction was included on Base within 3 minutes of doing the transaction on Ethereum L1. Pretty impressive.

Out of the box they support OP Mainnet, Base, Unichain, Ink, World Chain, Zora and a few others. It is great to see that the UX to do even the more exotic things on rollups have improved massively.

</details>
