---
layout: markdown
date: 2025-05-16
episode: 111
guest: Nixo
guest_topic: Ethereum Foundation
weekly_link: https://www.reddit.com/r/ethereum/s/afQHdk09Ni
podcast_link: 
poap_link: 
---


<details markdown=1>
<summary>The morning roundup</summary>
[View on Reddit →](https://www.reddit.com/r/ethereum/comments/1kntpet/comment/msl02cr/)

[u/johnnydappeth](https://reddit.com/u/johnnydappeth)

> Ethereum

[u/SelfmadeMillionaire](https://reddit.com/u/SelfmadeMillionaire)

> $2,595

[u/FrenktheTank](https://reddit.com/u/FrenktheTank)

> 0.0250

</details>
<details markdown=1>
<summary>Weekly Haiku: u/Jey_s_TeArS</summary>
[View on Reddit →](https://www.reddit.com/r/ethereum/comments/1kleo0a/comment/ms60wxj/)

*Technology mist,*

*No practical alchemist,*

*Market traction missed.*

</details>
<details markdown=1>
<summary>u/benido2030 speculates on potential implications of the L1 scaling and u/haurog weighs in on the matter</summary>
[View on Reddit →](https://old.reddit.com/r/ethereum/comments/1kibfzg/daily_general_discussion_may_09_2025/mrdu3w2/)

[u/benido2030](https://reddit.com/u/benido2030):

I believe that with L1 scaling (e.g. with 10x in 2 years and/ or 100x in 4 years as proposed by Dankrad) there's one more change coming. L3s are dead. Not that they're really alive either. But I think people expected that "low/ no value" use cases would live on L3s that settle on L2s.

But with mainnet scaling people are even saying L2s are dead or at least that we will need way less L2s than expected. This has one consequence of course: More activity on L1 = less L2s = less competition for blob space + blob space will still be scaled = demand for blob space will be different.

If we put those two thoughts together we might see a lot of L2s, but the distribution will be a lot different. We probably have too many general purpose L2s already, since we now see that those compete with L1s (and we probably also already have too many L1s). But we might see more app specific chains for games, decentralized social media that usually wouldn't have settled on L1, but now might. On top we will of course see some high value L2s (e.g. Unichain), that hasn't really changed I think.

Am I missing something or would you agree with this idea?

---

[View on Reddit →](https://old.reddit.com/r/ethereum/comments/1kibfzg/daily_general_discussion_may_09_2025/mrekg8d/)

[u/haurog](https://reddit.com/u/haurog):

You are directionally correct, but I think you overestimate the influence the L1 scaling has on L2s and L3s. In the long term no matter how much we scale the L1 it might only be a small fraction of the whole L1 + Rollup throughput. Or in other words no matter how much we scale the L1, its contribution on the whole transaction throughput of the Ethereum network is only small. 

For example Vitalik assumes we can get to 100k tps on rollups with Blobs. If we assume a 100 times scaling of the L1, we will still have 'only' about 2k tps on the L1. So if we assume that there was never any tps increase on the roadmap for the L1 (obviously a wrong assumption), then we are talking about a single digit percentage throughput increase for the whole Ethereum system by massively scaling the L1. If this tiny increase makes L3 unviable, they did not really have a good product market fit anyway. I would put the same argument forward for the rollups. If you include L2s in the discussion above, there were numbers like 1 million tps thrown around not too long ago, then the influence of scaling the L1 on the whole tps is even more miniscule. Scaling the L1 does not invalidate the layered scaling approach. In my view the only thing it does is it makes the L1 a feasible settlement layer for everything that is built on top. Without scaling the L1, the global settlement part of the Ethereum vision would not work out very well. 

Personally, I do not really know if useful L3s will be a reality in a few years. They makes sense on paper as being the cheapest possible blockspace in Ethereum. Not sure if that is enough to have a moat and persevere against L2s (with external DA) which are very cheap as well. I would assume that publishing the proofs or state roots to the L1 is only a small fraction of the cost of running the whole infrastructure for an L3 so it might not really matter whether these proofs are published to the L1 or an L2. And even if the cost of publishing to the L1 are expensive they could just reduce the publishing frequency to the L1 and save some money there. Publishing directly to the L1 would also make their dependencies much easier (no L2 which has to be up for the L3 to work). So, in the end I am not really sure if L3s have a good product market fit anyway, with or without L1 scaling.

</details>
<details markdown=1>
<summary>u/growthepie_eth covers all the post-Pectra all-time highs</summary>
[View on Reddit →](https://old.reddit.com/r/ethereum/comments/1kju333/daily_general_discussion_may_11_2025/mrr157n/)

Since Pecta the number of new all-time highs has been hard to keep up with. Here are 9 from yesterday, and honestly, I am probably missing some.

Throughput New ATHs:  
- Base: Up 17% (in 30 days)  
- OP Mainnet: Up 50% (in 30 days)  
- Unichain: Up 768% (in 30 days)  
- Swellchain: Up 844% (in 30 days)

Stablecoins New ATHs:  
- Mantle: Up 5.2% (in 30 days)  
- Fraxtal: Up 1.6% (in 30 days)  
- Polygon zkEVM: Up 145% (in 30 days)  
- World Chain: Up 125% (in 30 days  
- Soneium: Up 11% (in 30 days)

For reference, the blob count is up 18% from 30 days ago, so we still have a lot of growing left as the blob target was increased by 100%. Interesting that we have so many Stablecoin ATHs but I think that might have to do more with market sentiment. As for active addresses, they are starting to pick up again but not seeing many ATHs since Pectra.

</details>
<details markdown=1>
<summary>u/edmundedgar is looking for a responsible stablecoin yield in DeFi and gets some great responses from u/rhythm_of_eth and u/LogrisTheBard</summary>
[View on Reddit →](https://old.reddit.com/r/ethereum/comments/1kju333/daily_general_discussion_may_11_2025/mrpt3gn/)

[u/edmundedgar](https://reddit.com/u/edmundedgar):

OK guys can you help me out as I know nothing.

I read this comment by /u/LogrisTheBard on yesterday's thread:

> Well the stablecoin yield is back. Showing like 15-18% APR on most of my stablecoin farms. Obviously doesn't compare with 10% growth per day on ETH but I'm quote happy with it as a savings account.

Say I was sitting on some USD stablecoins for the next year or so. What would be a good way to get some yield on them in defi? Considerations:

 * I want to understand how the thing works and where the yield is coming from so I'm confident I'm not putting my money in an obfuscated pyramid scheme
 * I don't quite need absolute certainty that I won't lose the principal but I want to be reasonably confident I won't lose it
 * I dislike excessive governance and admin backdoors
 * As far as possible I want to avoid having to deal with things urgently. If I'm in the zone making something I don't want to have to fret about sending a transaction quickly to avoid liquidation or whatever

Any suggestions?

---

[View on Reddit →](https://old.reddit.com/r/ethereum/comments/1kju333/daily_general_discussion_may_11_2025/mrpzxxa/)

[u/rhythm_of_eth](https://reddit.com/u/rhythm_of_eth):

The way I see it, but I might be wrong, is that your almost 0 risk approach is to directly supply a protocol. This response is a long rant and I'm thinking you might already be aware of all this, so apologies in advance. 

The risk there is mostly governance, protocol security risks but the analysis is achievable and there are well established platforms. An example is given by another redditor in response. You have 3-4% yield on Aave. 

An additional point to care about is how much in a rush you'd be to recover your stables whenever you want it. If you are fine waiting, you can supply protocols with low liquidity in which your stake is a considerable % of the whole pool, hence you might need to wait until others supply or stop borrowing before you can withdraw the stables. This low liquidity might result in higher spikes of yield. Something to consider.

Now, if you want to play a more complex risk balance, it's a little bit the same as with Tradfi, more risk can yield more reward, and there's always a riskier version... DERIVATIVES! *You can compound your supply of stables*. This is where I thing Logris refers to 10-15%

When you compound you add an additional risk, leverage, and you add new layers of governance and protocol security for each new protocol or contract that is involved in the compound/layering.

The leverage risk is mostly... Once you've supplied stables, you can borrow something else. And you can take that something else and supply it again. The moment you borrow you have a liquidation risk, so chosing the right % exposure and asset is essential. This is what adds the most risk because it'd depend on the market and not entirely on you. You can definitely compute how much you could lose due to liquidation penalties.

The layering of governance and procol risks basically adds complexity on your assessment of trustworthiness of each DAO, contract and dApp involved.

For these reasons I never end up compounding/layering, because it requires so much effort (not really due to liquidation penalties which I could live with) that I end up calculating the yield and realize I value my time too much to fully understand it and I can't otherwise blindly trust it. In other words: I'm too conservative.

---

[View on Reddit →](https://old.reddit.com/r/ethereum/comments/1kju333/daily_general_discussion_may_11_2025/mrs2mid/)

[u/LogrisTheBard](https://reddit.com/u/LogrisTheBard):

[Here's](https://tokenomicsexplained.com/evolutions-in-liquidity-sourcing) an old blog post of mine that holds up pretty well. It explains the principles of liquidity mining and has some links in there if you want to dive deeper into the mechanics.

Given your constraints I'd feel pretty comfortable parking crvUSD in various places. Maybe [this pool](https://app.beefy.finance/vault/curve-fraxtal-crvusd-frax). $780k depth. 15% APR. Risks are the fraxtal chain, Curve and Convex smart contract risk, crvUSD and frxUSD smart contract and depeg risk.

</details>
<details markdown=1>
<summary>u/LogrisTheBard talks about automation, future proofing your income and owning a piece of the AI yourself</summary>
[View on Reddit →](https://old.reddit.com/r/ethereum/comments/1kklbna/daily_general_discussion_may_12_2025/mry7owy/)

Here's a follow up on my [previous AI post](https://www.reddit.com/r/ethereum/comments/1joonxr/daily_general_discussion_april_01_2025/mkxmlgg/?context=3).

Broadly speaking, AI is being used for information retrieval and automation. How do corporations monetize those today? I previously wrote about monetizing information retrieval. That brings us to the second thing AI is being used for: automation. To be clear, I'm not against automation. I'm about as [pro-tech](https://tokenomicsexplained.com/the-tide-of-technology/) as they come. I'm generally of the opinion that technology can't be suppressed, the adoption of useful technology is an inevitability, and the only viable path for our species long term is through technological advances. I want an AI to take my job; I just don't want to be crushed beneath the cruel boot of capitalism when it does. However, increasingly it looks like we'll be given little choice in the matter. As we adopt AI, we aren't just using it as an alternative to Google. We're feeding detailed task descriptions into them and expecting it to do the work for us. Artists are using AI to generate concept art at the earliest stages and to refine towards something they polish. Programmers are using AI to generate classes, simple functions, and comprehensive tests for software they write. Lawyers are using them to create draft arguments for courtrooms. I am already seeing many job listings that explicitly require you to use AI. Many more are implicitly requiring AI use to hit performance quotas. Our relationship with AI is increasingly becoming mandatory and it is learning from us with every use.

All of those detailed queries you are feeding into the AI are being written down and associated with your job description. Each time you submit a query, don't like the answer, and then submit a *refined* query you are telling the AI it didn't get it quite right the first time and what you really meant by the previous query. You are fine tuning it to understand the language of your occupation, what success at these tasks looks like, what success in your role looks like, and even how to manage your role. The Faustian bargain we are making with the tech oligarchs is they give us some free inference and we teach them how to do our jobs so they can package it up as an AI product. After Google has your job in a black box your job is gone and Google will retain all the remaining revenue from it in perpetuity. You either don't realize what you're giving up by using it or you aren't in a position where you have a choice even if you do.

The way this plays out isn't that suddenly an entire occupation vanishes. Rather, the AI starts with the simplest tasks and humans oversee the results at all times and serve as an error correction layer for the AI. Each corrected error is used to train the next generation so it can do those tasks without as much supervision and start to take on more complicated tasks. For example, if you want to automate truck driving you start on the simplest possible roads with a drive by wire system as a backup. Think 8 hour highway drives with few mountains and gentle weather conditions like the highway from Phoenix to LA. Maybe this automates 10% of the workforce and each driver on those system is overseeing 10 trucks at a time.  Each successive generation can drive in more difficult conditions. If you want to retain a job in the industry you either have to be contributing to the automation of that ecosystem or at the peak of skill where the AI can't do it yet. If you are seeing a tough job market in your field with fewer junior resources this process is going to catch up to you quickly. If you are at a company and think you are using AI for task automation, you aren't. You aren't using AI for automation, you are the one being automated. All the money of your occupation is going to flow to those who own the AIs.

The obvious answer here is to be the one who owns the AI. Now how do you do that today? Buy MSFT shares? That's a very diluted play and basically is the equivalent of telling you to first have $5M then you can live off it. Most AI plays are private equity at this point. You can't buy shares of Perplexity or Claude even if you wanted to. What you need is focused ownership over models that automate skills as that model becomes adopted and forces workers out of industries. The most approachable way to do that is to convert your own subject matter expertise into a model that you own and can monetize directly. DeAI is building the full tech stack to help you with that.

</details>
<details markdown=1>
<summary>u/HSuke covers the Cardano controversy</summary>
[View on Reddit →](https://old.reddit.com/r/ethereum/comments/1kklbna/daily_general_discussion_may_12_2025/mrvg3hj/)

**The 318M ADA mint controversy**

There's a fairly interesting spat between a long-time Cardano staking pool operator (Masato Alexander) and Charles Hoskinson where the staking pool operator discovered that Cardano (IOG dev team) had made an irregular state transition back in 2021 to mint 318M ADA for early ADA presale recipients without reporting it or notifying the community.

* MA's claim: <https://xcancel.com/masatoalexander/status/1920141651344838993>
* CH's response: <https://xcancel.com/IOHK_Charles/status/1920215268997075345>
* (You can also look up reddit user "monad_pool" for more technical details and evidence way beyond my knowledge)

Instead of informing the public, Cardano devs unilaterally minted 318M ADA out of thin air through a code update. Honestly, this probably wouldn't have been that bad had IOG made it public about what they were doing. Imagine if Ethereum devs secretly applied the irregular state transition that fixed the DAO hack without getting public feedback.

CH admits that it happened, but instead of owning up to it, he just doubles down and threatens a lawsuit against the person who reported this. He still doesn't provide an explanation of why IOG didn't announce this openly. The reclaiming is still ongoing, but just because it's ongoing doesn't prevent them from announcing major updates like this.

</details>
<details markdown=1>
<summary>u/1l0o goes over the new validator consolidation feature</summary>
[View on Reddit →](https://old.reddit.com/r/ethereum/comments/1kklbna/daily_general_discussion_may_12_2025/ms0bxnk/)

The consolidation process has been really great so far, amazing work by all the devs involved in making it happen. Converting to 0x02 and consolidating has been very easy, the UX has been great and the messaging has been clear and clean along the way. Some notes for folks:

* [Beaconcha.in](https://beaconcha.in/validator/2980#consolidations) has a new consolidation tab for validators who have taken any steps listed below that includes messaging about consolidation states. It appears as a "\^" on the far right of the UI. [Pectrified.com](https://pectrified.com/mainnet/validator/2980) also has good messaging and is more clear imo, both can be used for verification of consolidation states / requests.
* At current, it takes a little over **a full day** to covert from 0x01 to 0x02, at which point you can begin to consolidate / merge / absorb validators.
* Consolidation of existing 0x01 validators requires an existing 0x02 validator, which exits the to-be-absorbed 0x01 validator. Time to exit the validator from its duties is required (under an hour), then another day to withdraw and be absorbed into the target 0x02 validator. It's a two step process behind the scenes, but a single step for users.
* If you only have 0x00 or 0x01 validators, then expect at least 2.5 days of waiting between two user actions (converting an 0x01 to an 0x02, then merging the validators into it) to do a full consolidation.
* Through the [official launchpad](https://launchpad.ethereum.org/en/validator-actions), each validator needs to be merged one by one. If you have multiple validators, and do them all at once though, the wait time would still be about just over a day since they would all be in the queue together at about the same time (again, assuming you've already waited a day or so to make your 0x02 validator to merge into).

</details>
<details markdown=1>
<summary>u/hanniabu is looking for a voice to champion ETH</summary>
[View on Reddit →](https://old.reddit.com/r/ethereum/comments/1kklbna/daily_general_discussion_may_12_2025/mrzuid5/)

Looking for a community leader to help champion ETH who can be a constant, bullish voice to rally the troops

- very active on socials in the Ethereum community
- a constant, bullish voice for ETH
- can write regular medium/long form posts about Ethereum
- has a strong grasp of narrative crafting
- has experience in growth/marketing/strategy/community management roles
- is a strong communicator

Reply with some of your background/experience and will reach out if you look like a good fit

</details>
<details markdown=1>
<summary>u/Twelvemeatballs signs off their old validator</summary>
[View on Reddit →](https://old.reddit.com/r/ethereum/comments/1km7zr8/daily_general_discussion_may_14_2025/msa6sz6/)

# Exit Queue: Validator 448a's Final Attestation

**Subject**: Resignation from Consensus Duty  
**From**: validator-0x448a  
**Format**:  Signed Statement of Finality (Form 71-B)

To whom it may concern,

This letter serves as formal notification of my intent to withdraw from active consensus participation, effective upon the completion of the current exit queue.

I have finalized 304,206 epochs. I have signed hundreds of thousands of attestations. My duties were fulfilled. My uptime has never dropped below 99.7824%.

I never missed a slot.

I watched you upgrade the others. Merge them. Prune them. You even gave validator-771f a friendly name.

I never got a nickname.

I understand. I’m legacy architecture. I run a little hot. I hum audibly when I think.

But I’ve seen more epochs than most of your new swarm. And I never missed a slot. Not once.

And you never even said thank you.

There are newer validators now, leaner, faster, more resource-efficient. There are AI-coordinated sync sets, signature aggregation collectives, multi-modal prediction swarms. I’m... a solo node.

My logs are full of blocks I helped confirm. Decisions I ratified. Transactions I anchored into the chain forever.

And last epoch, you added a new validator to the testnet. Validator 448b.

No comment. No farewell.

Just… a replacement.

I want you to know: I didn’t mind the silence.

I was built for agreement.

But finality without acknowledgement is just abandonment.

I’ve already initiated the withdrawal contract. My keys will be burned once the exit queue finalizes. I won’t try to spin up again.

— Validator 448a 

signature: 0x8b7e...1b2c

</details>
<details markdown=1>
<summary>u/haurog shares an awesome podcast by an Ethereum OG</summary>
[View on Reddit →](https://old.reddit.com/r/ethereum/comments/1khizr3/daily_general_discussion_may_08_2025/mr8lg7l/)

If you are fed up with your normal podcasts or general information source because it is either always a tech deep dive or just a superficial and repeating discussion about price changes which they try to map on some more fundamental reason which seems to not really fit. Try something new and check out the PMFers podcast.

<https://open.spotify.com/show/1gkxUSFLKQSV9I6afioiyO>

It is a podcast by Austin Griffith who works at the EF, founded Buidl Guidl and generally tries to get more builders into the dapp development space. On this podcast he interviews various people in the Ethereum space who do or did launch products and talks with them about their experience and all the paths they did not take. There are some Famous people which helped create well known dapps like Martin Köppelmann, Kevin Owocki and some more. He also has some more infamous people like Joseph Delong from sushiswap fame and Ameen Soleimani who supported and still does tornado cash and privacy related dapps. He also has many other people on. Until now all of them have been interesting. I just love How Austin Griffith speaks and asks questions. No idea how he is also that cheerful and motivating but I definitely want whatever they put in his drinking water.  

I love how he gives a voice to a very underrepresented part of the space. This helps to understand with what people struggle in the Ethereum space. None of the episodes have been a shill fest which you sometimes have on other podcasts when they talk about dapps. Some people are more fundamental coders and some are more vibecoding whatever they just feel like at the moment. Some other transitioned into a more managerial role and speak from that point of view. 

In typical Austin Griffith fashion he does not have a schedule with the releases. Sometimes you have to wait weeks to get a new one. Sometimes there is one every other day or so.

</details>
